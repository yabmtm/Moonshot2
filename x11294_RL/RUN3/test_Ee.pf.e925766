                      :-) GROMACS - mdrun_mpi, 2018.8 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen
    Par Bjelkmar    Aldert van Buuren   Rudi van Drunen     Anton Feenstra  
  Gerrit Groenhof    Aleksei Iupinov   Christoph Junghans   Anca Hamuraru   
 Vincent Hindriksen Dimitrios Karkoulis    Peter Kasson        Jiri Kraus    
  Carsten Kutzner      Per Larsson      Justin A. Lemkul    Viveca Lindahl  
  Magnus Lundborg   Pieter Meulenhoff    Erik Marklund      Teemu Murtola   
    Szilard Pall       Sander Pronk      Roland Schulz     Alexey Shvetsov  
   Michael Shirts     Alfons Sijbers     Peter Tieleman    Teemu Virolainen 
 Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      mdrun_mpi, version 2018.8
Executable:   /gpfs/opt/apps/gromacs-2018.8/bin/mdrun_mpi
Data prefix:  /gpfs/opt/apps/gromacs-2018.8
Working dir:  /work/tug27224/MOON2/projects/x11294_RL/RUN3
Command line:
  mdrun_mpi -maxh 47 -s prod.tpr

Compiled SIMD: SSE4.1, but for this host/run AVX2_256 might be better (see
log).
The current CPU can measure timings more accurately than the code in
mdrun_mpi was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding mdrun_mpi with the GMX_USE_RDTSCP=ON CMake option.
Reading file prod.tpr, VERSION 2018.8 (single precision)

Multiple energy groups is not implemented for GPUs, falling back to the CPU. For better performance, run on the GPU without energy groups and then do gmx mdrun -rerun option on the trajectory with an energy group .tpr file.

Will use 18 particle-particle and 10 PME only ranks
This is a guess, check the performance at the end of the log file

Using 28 MPI processes
Using 1 OpenMP thread per MPI process


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

NOTE: There are issues with expanded ensemble and certain combination of nstexpanded and nstcalcenergy, setting nstcalcenergy to 1
starting mdrun 'Generic title'
-1 steps, infinite ps.

Step 179915, time 719.66 (ps)  LINCS WARNING
relative constraint deviation after LINCS:
rms 0.000001, max 0.000021 (between atoms 2093 and 2091)
bonds that rotated more than 30 degrees:
 atom 1 atom 2  angle  previous, current, constraint length
   2061   2060   30.3    0.1090   0.1090      0.1090

Step 179920, time 719.68 (ps)  LINCS WARNING
relative constraint deviation after LINCS:
rms 0.000001, max 0.000009 (between atoms 2355 and 2357)
bonds that rotated more than 30 degrees:
 atom 1 atom 2  angle  previous, current, constraint length
   2061   2060   30.5    0.1090   0.1090      0.1090

Step 180050, time 720.2 (ps)  LINCS WARNING
relative constraint deviation after LINCS:
rms 0.009130, max 0.195063 (between atoms 2092 and 2091)
bonds that rotated more than 30 degrees:
 atom 1 atom 2  angle  previous, current, constraint length
   2061   2060   68.9    0.1090   0.1090      0.1090
   2092   2091  100.3    0.1090   0.1303      0.1090

Step 180051, time 720.204 (ps)  LINCS WARNING
relative constraint deviation after LINCS:
rms 5.447699, max 96.509598 (between atoms 2092 and 2091)
bonds that rotated more than 30 degrees:
 atom 1 atom 2  angle  previous, current, constraint length
   2061   2060   30.1    0.1090   0.1090      0.1090
   2092   2091  143.7    0.1303  10.6285      0.1090
   2093   2091   35.9    0.1091   0.1140      0.1090
   2089   2088  101.5    0.1090   0.5459      0.1090
   2090   2088   83.8    0.1090   7.1878      0.1090
Wrote pdb files with previous and current coordinates

WARNING: Listed nonbonded interaction between particles 2080 and 2092
at distance 10.389 which is larger than the table limit 2.005 nm.

This is likely either a 1,4 interaction, or a listed interaction inside
a smaller molecule you are decoupling during a free energy calculation.
Since interactions at distances beyond the table cannot be computed,
they are skipped until they are inside the table limit again. You will
only see this message once, even if it occurs for several interactions.

IMPORTANT: This should not happen in a stable simulation, so there is
probably something wrong with your system. Only change the table-extension
distance in the mdp file if you are really sure that is the reason.



Step 180052, time 720.208 (ps)  LINCS WARNING
relative constraint deviation after LINCS:
rms 89.834770, max 1609.254639 (between atoms 3033 and 3032)
bonds that rotated more than 30 degrees:
 atom 1 atom 2  angle  previous, current, constraint length
   2061   2060  123.0    0.1090   0.2003      0.1090
   2093   2091  117.9    0.1140   0.2327      0.1090
   2089   2088   81.0    0.5459  72.3169      0.1090
   2086   2085   96.4    0.1090  81.3102      0.1090
   2087   2085  111.8    0.1090   0.2939      0.1090
   3033   3032   90.4    0.1090 175.5178      0.1090
   3031   3030   37.5    0.1090   0.1090      0.1090
Wrote pdb files with previous and current coordinates

-------------------------------------------------------
Program:     mdrun_mpi, version 2018.8
Source file: src/gromacs/ewald/pme-redistribute.cpp (line 282)
MPI rank:    2 (out of 28)

Fatal error:
1 particles communicated to PME rank 0 are more than 2/3 times the cut-off out
of the domain decomposition cell of their charge group in dimension x.
This usually means that your system is not well equilibrated.

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
